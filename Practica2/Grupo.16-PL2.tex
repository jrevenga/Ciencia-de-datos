\documentclass[a4paper, 12pt]{article}
\usepackage[spanish]{babel}
\usepackage[hmargin=2.5cm,vmargin=2.5cm]{geometry}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{fancyhdr}

\setlength{\arrayrulewidth}{0.4mm}

\newcommand{\HRule}{\rule{\linewidth}{0.5mm}}

\usepackage{Sweave}
\begin{document}
\input{Grupo.16-PL2-concordance}
	\begin{titlepage}
		\begin{center}
			\includegraphics[width=0.5\textwidth]{logoUAH.png}~\\[2cm]
			
			\textsc{\Large \\Fundamentos de la Ciencia de Datos}\\[2cm]
			
			\HRule \\[0.4cm]
			{\LARGE \bfseries Prueba de Laboratorio 2 (PL2) \\[0.4cm]}
			\HRule \\[2cm]
			
			\large\textbf{Jorge Revenga Martín de Vidales}\\
			\large\textbf{Ángel Salgado Aldao}\\
			\large\textbf{Adrián García}\\
			\large\textbf{}\\ Grado en Ingeniería Informática \\ Universidad de Alcalá
			
			\vfill
			
			{\large \today}
		\end{center}
	\end{titlepage}
	% Configura los encabezados y pies de página
	\pagestyle{fancy}
	\fancyhf{} % Limpia todos los encabezados y pies de página actuales
	% Encabezado
	\fancyhead[RO,LE]{\textit{Fundamentos de la ciencia de datos}}
	\fancyhead[LO,RE]{\textit{Prueba de Laboratorio 2 (PL2)}}
	% Pie de página
	\fancyfoot[LO,RE]{\textit{Universidad de Alcalá}}
	\fancyfoot[RO,LE]{\thepage}  % Número de página en la esquina inferior derecha
	\newpage
	
	\thispagestyle{plain}
	\tableofcontents
	
	
	\newpage
	
	\section{Introducción - Consideraciones previas}
	
	\subsection{Uso de RStudio}
	Para utilizar una función en R se escribe el nombre de la función, seguido de los parámetros de entrada entre paréntesis e.g.: \texttt{función(parámetros)}
	
	\begin{itemize}
		\item[-]Función \texttt{contributors()}: Muestra los creadores del programa (R)
		
		\item[-]Función \texttt{help()}: Abre un HTML con información sobre la función \texttt{help()} o de la función entre paréntesis de haberla. Para todas las funciones que programemos (para todas las que existan) en R debe poder usarse la función \texttt{help()}.
		
		En el archivo HTML se distinguen varios elementos:
		
		\begin{itemize}
			\item función \{paquete\}: la función de la que se obtiene información seguida del paquete al que pertenece.
			\item Description: descripción de la función.
			\item Usage: aparece la función y todos los argumentos que se le pueden introducir.
			\item Arguments: Explicación de los argumentos o parámetros.
			\item Details: Detalles adicionales de la función.
			\item Offline help: Ayuda sin conexión.
			\item Note: Nota del autor.
			\item References: Referencias.
			\item Examples: Ejemplos de uso de la función.
		\end{itemize}
		
		\item[-]Función \texttt{getwd()} se utiliza para obtener el directorio de trabajo actual (working directory).
		
		\item[-]Función \texttt{setwd(\string"C:/…")} permite cambiar el nuevo directorio de trabajo en el que queramos trabajar.
		
		\item[-] \texttt{help.start()}: Manda a un compendio de todas las ayudas disponibles para trabajar con R.
		
		\item[-]Función \texttt{list.files()}: Muestra todos los archivos en el directorio. \texttt{dir()} hace lo mismo. 
	\end{itemize}
	
	\subsection{Introducción de datos de Excel/CSV en R}
	Para poder leer archivos .xlsx y .csv seguimos un par de pasos:

	\begin{itemize}
		\item[-] Archivos .csv:
		\begin{itemize}
			\item Primero se crea el archivo .csv y se introducen los datos por filas (los datos de la fila i, se escriben en la celda (i,1)) y separando dichos datos con un delimitador ('',´´ o '';´´).
			\item Después se usa la función \texttt{read.csv(''ruta del archivo´´, sep = '';´´)}, la cual es parte del paquete base de R, para leer los datos del archivo. Recibe como parámetros la ruta del archivo y el delimitador que usa para separar los datos.
		\end{itemize}
		
		\item[-] Archivos .xlsx:
		\begin{itemize}
			\item Primero se crea el archivo .xlsc y se introducen los datos en forma de matriz, escribiendo cada dato en una celda.
			\item Después se carga la librería \emph{openxlsx} y se usa la función \texttt{read\_excel(''ruta del archivo´´)}, introduciendo la ruta del archivo por parámetro.
		\end{itemize}
	\end{itemize}
	
	
  \newpage
	
	\section{Ejercicios con ayuda del profesor}
	Realización de cuatro ejercicios con ayuda del profesor en los que se van a realizar, utilizando el entorno R, dos análisis de clasificación no supervisada y dos análisis de clasificación supervisada, aplicando todos los conceptos teóricos vistos en cada lección.

	\subsection{Análisis de clasificación no supervisada}
	
	\subsubsection{k-Means}
	
	El primer conjunto de datos, que se empleará para realizar el análisis de clasificación no supervisada con k-Means, estará formado por las siguientes 8 calificaciones de estudiantes: 1.\{4, 4\}; 2.\{3, 5\}; 3.\{1, 2\}; 4.\{5, 5\}; 5.\{0, 1\}; 6.\{2, 2\}; 7.\{4, 5\}; 8.\{2, 1\}, donde las características de las calificaciones son: \{Teoría, Laboratorio\}.
	
	\paragraph{Solución:}
	\begin{itemize}
		\item \texttt{m<-matrix(c(4,4, 3,5, 1,2, 5,5, 0,1, 2,2, 4,5, 2,1),2,8)}: 
		Introducción de los datos
\begin{Schunk}
\begin{Sinput}
> m<-matrix(c(4,4, 3,5, 1,2, 5,5, 0,1, 2,2, 4,5, 2,1),2,8)
> (m<-t(m))
\end{Sinput}
\begin{Soutput}
     [,1] [,2]
[1,]    4    4
[2,]    3    5
[3,]    1    2
[4,]    5    5
[5,]    0    1
[6,]    2    2
[7,]    4    5
[8,]    2    1
\end{Soutput}
\end{Schunk}
		\item \texttt{c<-matrix(c(0,1,2,2),2,2)}: 
		La función de k-means viene en los paquetes por defecto, toma la matriz y los centroides iniciales, este código crea los centriodes
\begin{Schunk}
\begin{Sinput}
> c<-matrix(c(0,1,2,2),2,2)
> (c<-t(c))
\end{Sinput}
\begin{Soutput}
     [,1] [,2]
[1,]    0    1
[2,]    2    2
\end{Soutput}
\end{Schunk}
		\item \texttt{(clasificacionns=(kmeans(m,c,4)))}: 
		Almacena el resultado en la variable clasificacionns(no supervisada)
\begin{Schunk}
\begin{Sinput}
> (clasificacionns=(kmeans(m,c,4)))
\end{Sinput}
\begin{Soutput}
K-means clustering with 2 clusters of sizes 4, 4

Cluster means:
  [,1] [,2]
1 1.25 1.50
2 4.00 4.75

Clustering vector:
[1] 2 2 1 2 1 1 2 1

Within cluster sum of squares by cluster:
[1] 3.75 2.75
 (between_SS / total_SS =  84.8 %)

Available components:

[1] "cluster"      "centers"      "totss"        "withinss"     "tot.withinss"
[6] "betweenss"    "size"         "iter"         "ifault"      
\end{Soutput}
\end{Schunk}
		\item \texttt{}: 
		Extraemos el vector de la solución de kmeans  (usando el dolar obtenemos la variable cluster) y se la añadimos a la matriz
\begin{Schunk}
\begin{Sinput}
> (m=cbind(clasificacionns$cluster,m))
\end{Sinput}
\begin{Soutput}
     [,1] [,2] [,3]
[1,]    2    4    4
[2,]    2    3    5
[3,]    1    1    2
[4,]    2    5    5
[5,]    1    0    1
[6,]    1    2    2
[7,]    2    4    5
[8,]    1    2    1
\end{Soutput}
\end{Schunk}
		\item \texttt{}: 
		Separa la matriz por clusters
\begin{Schunk}
\begin{Sinput}
> mc1=subset(m,m[,1]==1)
> mc2=subset(m,m[,1]==2)
> mc1
\end{Sinput}
\begin{Soutput}
     [,1] [,2] [,3]
[1,]    1    1    2
[2,]    1    0    1
[3,]    1    2    2
[4,]    1    2    1
\end{Soutput}
\begin{Sinput}
> mc2
\end{Sinput}
\begin{Soutput}
     [,1] [,2] [,3]
[1,]    2    4    4
[2,]    2    3    5
[3,]    2    5    5
[4,]    2    4    5
\end{Soutput}
\end{Schunk}
		\item \texttt{}: 
		Quitamos la primera columna de ambas ya que sólo indica el cluster
\begin{Schunk}
\begin{Sinput}
> (mc1=mc1[,-1])
\end{Sinput}
\begin{Soutput}
     [,1] [,2]
[1,]    1    2
[2,]    0    1
[3,]    2    2
[4,]    2    1
\end{Soutput}
\begin{Sinput}
> (mc2=mc2[,-1])
\end{Sinput}
\begin{Soutput}
     [,1] [,2]
[1,]    4    4
[2,]    3    5
[3,]    5    5
[4,]    4    5
\end{Soutput}
\end{Schunk}
		\item \texttt{}: 
		Finalmente, vemos los datos separados en tablas por el cluster al que pertenecen.

	\end{itemize}
	
	\newpage
	
	\subsubsection{Clusterización Jerárquica Aglomerativa}
	
	El segundo conjunto de datos, que se empleará para realizar el análisis de clasificación no supervisada con Clusterización Jerárquica Aglomerativa, estará formado por 6 calificaciones de estudiantes: 1.\{0.89, 2.94\}; 2.\{4.36, 5.21\}; 3.\{3.75, 1.12\}; 4.\{6.25, 3.14\}; 5.\{4.1, 1.8\}; 6.\{3.9, 4.27\}.
	
	\paragraph{Solución}

	\begin{itemize}
\begin{Schunk}
\begin{Sinput}
> library(LearnClust)
> search()
\end{Sinput}
\begin{Soutput}
 [1] ".GlobalEnv"         "package:LearnClust" "package:magick"    
 [4] "package:arules"     "package:Matrix"     "package:stats"     
 [7] "package:graphics"   "package:grDevices"  "package:utils"     
[10] "package:datasets"   "package:methods"    "Autoloads"         
[13] "package:base"      
\end{Soutput}
\end{Schunk}
		\item \texttt{}: 
		Explicacion
\begin{Schunk}
\begin{Sinput}
> m<-matrix(c(0.89,2.94, 4.36,5.21, 3.75,1.12, 6.25,3.14, 4.1,1.8, 3.9,4.27),2,6)
> (m<-t(m))
\end{Sinput}
\begin{Soutput}
     [,1] [,2]
[1,] 0.89 2.94
[2,] 4.36 5.21
[3,] 3.75 1.12
[4,] 6.25 3.14
[5,] 4.10 1.80
[6,] 3.90 4.27
\end{Soutput}
\end{Schunk}
		\item \texttt{}: 
		Explicacion
\begin{Schunk}
\begin{Sinput}
> agglomerativeHC(m,'EUC','MIN')
\end{Sinput}
\begin{Soutput}
$dendrogram
Number of objects: 6 


$clusters
$clusters[[1]]
    X1   X2
1 0.89 2.94

$clusters[[2]]
    X1   X2
1 4.36 5.21

$clusters[[3]]
    X1   X2
1 3.75 1.12

$clusters[[4]]
    X1   X2
1 6.25 3.14

$clusters[[5]]
   X1  X2
1 4.1 1.8

$clusters[[6]]
   X1   X2
1 3.9 4.27

$clusters[[7]]
    X1   X2
1 3.75 1.12
2 4.10 1.80

$clusters[[8]]
    X1   X2
1 4.36 5.21
2 3.90 4.27

$clusters[[9]]
    X1   X2
1 3.75 1.12
2 4.10 1.80
3 4.36 5.21
4 3.90 4.27

$clusters[[10]]
    X1   X2
1 6.25 3.14
2 3.75 1.12
3 4.10 1.80
4 4.36 5.21
5 3.90 4.27

$clusters[[11]]
    X1   X2
1 0.89 2.94
2 6.25 3.14
3 3.75 1.12
4 4.10 1.80
5 4.36 5.21
6 3.90 4.27


$groupedClusters
  cluster1 cluster2
1        3        5
2        2        6
3        7        8
4        4        9
5        1       10
\end{Soutput}
\end{Schunk}
		\item \texttt{}: 
		Explicacion
\begin{Schunk}
\begin{Sinput}
> agglomerativeHC.details(m,'EUC','MIN')
\end{Sinput}
\begin{Soutput}
[[1]]
     [,1] [,2] [,3]
[1,] 0.89 2.94    1

[[2]]
     [,1] [,2] [,3]
[1,] 4.36 5.21    1

[[3]]
     [,1] [,2] [,3]
[1,] 3.75 1.12    1

[[4]]
     [,1] [,2] [,3]
[1,] 6.25 3.14    1

[[5]]
     [,1] [,2] [,3]
[1,]  4.1  1.8    1

[[6]]
     [,1] [,2] [,3]
[1,]  3.9 4.27    1

         [,1]     [,2]      [,3]     [,4]      [,5]     [,6]
[1,] 0.000000 4.146541 3.3899853 5.363730 3.4064204 3.290745
[2,] 4.146541 0.000000 4.1352388 2.803034 3.4198977 1.046518
[3,] 3.389985 4.135239 0.0000000 3.214094 0.7647876 3.153569
[4,] 5.363730 2.803034 3.2140940 0.000000 2.5333969 2.607566
[5,] 3.406420 3.419898 0.7647876 2.533397 0.0000000 2.478084
[6,] 3.290745 1.046518 3.1535694 2.607566 2.4780839 0.000000
    X1   X2
1 3.75 1.12
2 4.10 1.80
         [,1]     [,2] [,3]     [,4] [,5]     [,6]     [,7]
[1,] 0.000000 4.146541    0 5.363730    0 3.290745 3.389985
[2,] 4.146541 0.000000    0 2.803034    0 1.046518 3.419898
[3,] 0.000000 0.000000    0 0.000000    0 0.000000 0.000000
[4,] 5.363730 2.803034    0 0.000000    0 2.607566 2.533397
[5,] 0.000000 0.000000    0 0.000000    0 0.000000 0.000000
[6,] 3.290745 1.046518    0 2.607566    0 0.000000 2.478084
[7,] 3.389985 3.419898    0 2.533397    0 2.478084 0.000000
    X1   X2
1 4.36 5.21
2 3.90 4.27
         [,1] [,2] [,3]     [,4] [,5] [,6]     [,7]     [,8]
[1,] 0.000000    0    0 5.363730    0    0 3.389985 3.290745
[2,] 0.000000    0    0 0.000000    0    0 0.000000 0.000000
[3,] 0.000000    0    0 0.000000    0    0 0.000000 0.000000
[4,] 5.363730    0    0 0.000000    0    0 2.533397 2.607566
[5,] 0.000000    0    0 0.000000    0    0 0.000000 0.000000
[6,] 0.000000    0    0 0.000000    0    0 0.000000 0.000000
[7,] 3.389985    0    0 2.533397    0    0 0.000000 2.478084
[8,] 3.290745    0    0 2.607566    0    0 2.478084 0.000000
    X1   X2
1 3.75 1.12
2 4.10 1.80
3 4.36 5.21
4 3.90 4.27
          [,1] [,2] [,3]     [,4] [,5] [,6] [,7] [,8]     [,9]
 [1,] 0.000000    0    0 5.363730    0    0    0    0 3.290745
 [2,] 0.000000    0    0 0.000000    0    0    0    0 0.000000
 [3,] 0.000000    0    0 0.000000    0    0    0    0 0.000000
 [4,] 5.363730    0    0 0.000000    0    0    0    0 2.533397
 [5,] 0.000000    0    0 0.000000    0    0    0    0 0.000000
 [6,] 0.000000    0    0 0.000000    0    0    0    0 0.000000
 [7,] 0.000000    0    0 0.000000    0    0    0    0 0.000000
 [8,] 0.000000    0    0 0.000000    0    0    0    0 0.000000
 [9,] 3.290745    0    0 2.533397    0    0    0    0 0.000000
    X1   X2
1 6.25 3.14
2 3.75 1.12
3 4.10 1.80
4 4.36 5.21
5 3.90 4.27
          [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9]    [,10]
 [1,] 0.000000    0    0    0    0    0    0    0    0 3.290745
 [2,] 0.000000    0    0    0    0    0    0    0    0 0.000000
 [3,] 0.000000    0    0    0    0    0    0    0    0 0.000000
 [4,] 0.000000    0    0    0    0    0    0    0    0 0.000000
 [5,] 0.000000    0    0    0    0    0    0    0    0 0.000000
 [6,] 0.000000    0    0    0    0    0    0    0    0 0.000000
 [7,] 0.000000    0    0    0    0    0    0    0    0 0.000000
 [8,] 0.000000    0    0    0    0    0    0    0    0 0.000000
 [9,] 0.000000    0    0    0    0    0    0    0    0 0.000000
[10,] 3.290745    0    0    0    0    0    0    0    0 0.000000
    X1   X2
1 0.89 2.94
2 6.25 3.14
3 3.75 1.12
4 4.10 1.80
5 4.36 5.21
6 3.90 4.27
\end{Soutput}
\end{Schunk}
		\item \texttt{}: 
		Explicacion
\begin{Schunk}
\begin{Sinput}
> agglomerativeHC.details(m,'EUC','MAX')
\end{Sinput}
\begin{Soutput}
[[1]]
     [,1] [,2] [,3]
[1,] 0.89 2.94    1

[[2]]
     [,1] [,2] [,3]
[1,] 4.36 5.21    1

[[3]]
     [,1] [,2] [,3]
[1,] 3.75 1.12    1

[[4]]
     [,1] [,2] [,3]
[1,] 6.25 3.14    1

[[5]]
     [,1] [,2] [,3]
[1,]  4.1  1.8    1

[[6]]
     [,1] [,2] [,3]
[1,]  3.9 4.27    1

         [,1]     [,2]      [,3]     [,4]      [,5]     [,6]
[1,] 0.000000 4.146541 3.3899853 5.363730 3.4064204 3.290745
[2,] 4.146541 0.000000 4.1352388 2.803034 3.4198977 1.046518
[3,] 3.389985 4.135239 0.0000000 3.214094 0.7647876 3.153569
[4,] 5.363730 2.803034 3.2140940 0.000000 2.5333969 2.607566
[5,] 3.406420 3.419898 0.7647876 2.533397 0.0000000 2.478084
[6,] 3.290745 1.046518 3.1535694 2.607566 2.4780839 0.000000
    X1   X2
1 3.75 1.12
2 4.10 1.80
         [,1]     [,2] [,3]     [,4] [,5]     [,6]     [,7]
[1,] 0.000000 4.146541    0 5.363730    0 3.290745 3.406420
[2,] 4.146541 0.000000    0 2.803034    0 1.046518 4.135239
[3,] 0.000000 0.000000    0 0.000000    0 0.000000 0.000000
[4,] 5.363730 2.803034    0 0.000000    0 2.607566 3.214094
[5,] 0.000000 0.000000    0 0.000000    0 0.000000 0.000000
[6,] 3.290745 1.046518    0 2.607566    0 0.000000 3.153569
[7,] 3.406420 4.135239    0 3.214094    0 3.153569 0.000000
    X1   X2
1 4.36 5.21
2 3.90 4.27
         [,1] [,2] [,3]     [,4] [,5] [,6]     [,7]     [,8]
[1,] 0.000000    0    0 5.363730    0    0 3.406420 4.146541
[2,] 0.000000    0    0 0.000000    0    0 0.000000 0.000000
[3,] 0.000000    0    0 0.000000    0    0 0.000000 0.000000
[4,] 5.363730    0    0 0.000000    0    0 3.214094 2.803034
[5,] 0.000000    0    0 0.000000    0    0 0.000000 0.000000
[6,] 0.000000    0    0 0.000000    0    0 0.000000 0.000000
[7,] 3.406420    0    0 3.214094    0    0 0.000000 4.135239
[8,] 4.146541    0    0 2.803034    0    0 4.135239 0.000000
    X1   X2
1 6.25 3.14
2 4.36 5.21
3 3.90 4.27
         [,1] [,2] [,3] [,4] [,5] [,6]     [,7] [,8]     [,9]
 [1,] 0.00000    0    0    0    0    0 3.406420    0 5.363730
 [2,] 0.00000    0    0    0    0    0 0.000000    0 0.000000
 [3,] 0.00000    0    0    0    0    0 0.000000    0 0.000000
 [4,] 0.00000    0    0    0    0    0 0.000000    0 0.000000
 [5,] 0.00000    0    0    0    0    0 0.000000    0 0.000000
 [6,] 0.00000    0    0    0    0    0 0.000000    0 0.000000
 [7,] 3.40642    0    0    0    0    0 0.000000    0 4.135239
 [8,] 0.00000    0    0    0    0    0 0.000000    0 0.000000
 [9,] 5.36373    0    0    0    0    0 4.135239    0 0.000000
    X1   X2
1 0.89 2.94
2 3.75 1.12
3 4.10 1.80
      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8]    [,9]   [,10]
 [1,]    0    0    0    0    0    0    0    0 0.00000 0.00000
 [2,]    0    0    0    0    0    0    0    0 0.00000 0.00000
 [3,]    0    0    0    0    0    0    0    0 0.00000 0.00000
 [4,]    0    0    0    0    0    0    0    0 0.00000 0.00000
 [5,]    0    0    0    0    0    0    0    0 0.00000 0.00000
 [6,]    0    0    0    0    0    0    0    0 0.00000 0.00000
 [7,]    0    0    0    0    0    0    0    0 0.00000 0.00000
 [8,]    0    0    0    0    0    0    0    0 0.00000 0.00000
 [9,]    0    0    0    0    0    0    0    0 0.00000 5.36373
[10,]    0    0    0    0    0    0    0    0 5.36373 0.00000
    X1   X2
1 6.25 3.14
2 4.36 5.21
3 3.90 4.27
4 0.89 2.94
5 3.75 1.12
6 4.10 1.80
\end{Soutput}
\end{Schunk}
		\item \texttt{}: 
		Explicacion
\begin{Schunk}
\begin{Sinput}
> 
\end{Sinput}
\end{Schunk}

	\end{itemize}

	\newpage

	\subsection{Análisis de clasificación supervisada}
	
	\subsubsection{Árboles de decisión}
	
	El tercer conjunto de datos, que se empleará para realizar el análisis de clasificación supervisada utilizando árboles de decisión, estará formado por las siguientes 9 calificaciones de estudiantes: 1.\{A,A,B,Ap\}; 2.\{A,B,D,Ss\}; 3.\{D,D,C,Ss\}; 4.\{D,D,A,Ss\}; 5.\{B,C,B,Ss\}; 6.\{C,B,B,Ap\}; 7.\{B,B,A,Ap\}; 8.\{C,D,C,Ss\}; 9.\{B,A,C,Ss\}, donde las características de las calificaciones son: \{Teoría, Laboratorio, Prácticas, Calificación Global).

	\paragraph{Solución:}
	\begin{itemize}
	  \item Primero creamos un txt con los datos del problema en cuestión, para después mediante el uso del paquete ''rpart´´ realizar la clasificación.
\begin{Schunk}
\begin{Sinput}
> library("rpart")
> (muestra=read.table("calificaciones.txt"))
\end{Sinput}
\begin{Soutput}
  T L P C.G
1 A A B  Ap
2 A B D  Ss
3 D D C  Ss
4 D D A  Ss
5 B C B  Ss
6 C B B  Ap
7 B B A  Ap
8 C D C  Ss
9 B A C  Ss
\end{Soutput}
\end{Schunk}

    	  \item Acto seguido hacemos uso de la función que viene incluida en el paquete llamada \texttt{rpart()}, la cual recibe la variable dependiente \texttt{C.G} y las variables predictoras \texttt{T,L,P}, el conjunto de datos sobre el que realiza la clasificación \texttt{data=muestra}, con \texttt{method=class} indica que se está ajustando un modelo de clasificación y el número mínimo de observaciones \texttt{minsplit=1}.
\begin{Schunk}
\begin{Sinput}
> clasificacion=rpart(C.G~T+L+P, data=muestra, method="class", minsplit=1)
\end{Sinput}
\end{Schunk}

    	  \item Esta función hace la clasificación pertinente y devuelve en forma de árbol la solución con los diferentes nodos, además indica que nodos representan un nodo final usando el símbolo ''*´´.
    
  	\end{itemize}
	
	\subsubsection{Regresión}
	
	El cuarto conjunto de datos, que se empleará para realizar el análisis de clasificación supervisada utilizando regresión, estará formado por los siguientes 4 radios ecuatoriales y densidades de los planetas interiores: \{Mercurio,2.4,5.4; Venus,6.1,5.2; Tierra,6.4,5.5; Marte,3.4,3.9\}
	
	\paragraph{Solución:}
	\begin{itemize}
	  \item El primer paso es crear un txt con los datos necesarios, para después hacer uso de él en el análisis de regresión
\begin{Schunk}
\begin{Sinput}
> (muestra=read.table("planetas.txt"))
\end{Sinput}
\begin{Soutput}
           R   D
Mercurio 2.4 5.4
Venus    6.1 5.2
Tierra   6.4 5.5
Marte    3.4 3.9
\end{Soutput}
\end{Schunk}
	
	  \item Después usando la función \texttt{lm()}, se calcula la regresión indicando la relación entre la variable dependiente \texttt{D} y la variable independiente{R} y el conjunto de datos en el que se encuentran dichas variables \texttt{data=muestra}.
\begin{Schunk}
\begin{Sinput}
> regresion=lm(D~R, data=muestra)  
\end{Sinput}
\end{Schunk}

    	  \item Además de esto, gracias a la función \texttt{summary()}, podemos obtener el valor de R-squared y observar la bonanza de la regresión.
\begin{Schunk}
\begin{Sinput}
> summary(regresion)
\end{Sinput}
\begin{Soutput}
Call:
lm(formula = D ~ R, data = muestra)

Residuals:
Mercurio    Venus   Tierra    Marte 
 0.70312 -0.01253  0.24566 -0.93624 

Coefficients:
            Estimate Std. Error t value Pr(>|t|)  
(Intercept)   4.3624     1.2050   3.620   0.0685 .
R             0.1394     0.2466   0.565   0.6289  
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 0.846 on 2 degrees of freedom
Multiple R-squared:  0.1377,	Adjusted R-squared:  -0.2935 
F-statistic: 0.3193 on 1 and 2 DF,  p-value: 0.6289
\end{Soutput}
\end{Schunk}

    	  \item Otro uso que podemos hacer de la función \texttt{summary()}, es analizar los outliers de la siguiente forma. En la cual obtenemos los residuos, para después hacer la media de los mismos y con un bucle for comprobar cual de todos ellos es un outlier en caso de que los hubiese.
\begin{Schunk}
\begin{Sinput}
> (res=summary(regresion)$residuals)
\end{Sinput}
\begin{Soutput}
   Mercurio       Venus      Tierra       Marte 
 0.70312301 -0.01253452  0.24565541 -0.93624389 
\end{Soutput}
\begin{Sinput}
> sr=sqrt(sum(res^2)/4)
> for (i in 1:length(res)){if (res[i]>3*sr){print("el suceso"); print(res[i]); print("es un suceso anómalo")}}
\end{Sinput}
\end{Schunk}
  	\end{itemize}
	
	\newpage
	
	\section{Ejercicios de forma autónoma}
	
	Realización de cuatro ejercicios de forma autónoma por cada grupo de estudiantes en los que se van a realizar, utilizando el entorno R, dos análisis de clasificación no supervisada y dos análisis de clasificación supervisada, aplicando todos los conceptos teóricos vistos en cada lección.
	
	\subsection{Análisis de clasificación no supervisada}
	
	\subsubsection{K-means}
	
	El primer conjunto de datos, que se empleará para realizar el análisis de clasificación no supervisada con K-means, estará formado por los siguientes 15 valores de velocidades de respuesta y temperaturas normalizadas de un microprocesador \{Velocidad, Temperatura\}: 1.\{3.5, 4.5\}; 2.\{0.75, 3.25\}; 3.\{0, 3\}; 4.\{1.75, 0.75\}; 5.\{3, 3.75\}; 6.\{3.75, 4.5\}; 7.\{1.25, 0.75\}; 8.\{0.25, 3\}; 9.\{3.5, 4.25\}; 10.\{1.5, 0.5\}; 11.\{1, 1\}; 12.\{3, 4\}; 13.\{0.5, 3\}; 14.\{2, 0.25\}; 15.\{0, 2.5\}. Del análisis visual de los datos se ha concluido que hay una alta probabilidad que sean tres clusters.
	
	\paragraph{Solución:}

	\begin{itemize}
		\item \texttt{datos <- read.csv("Datos2.1.csv")}: Se leen los datos del archivo .csv y se guardan en la variable "datos".
		\item \texttt{clasificacionns <- kmedias(datos, num\_clusters = 3, mostrar\_proceso = TRUE)}: Llamar a nuestra función que clasifica con K-means, seleccionando el número de clusters a generar (La función creada genera 2 en caso de no especificarse) y que nos muestre el proceso de clasificación de los clusters.
	
\begin{Schunk}
\begin{Sinput}
> #cargamos la función
> source("kmedias.R")
> datos <- read.csv("Datos2.1.csv")
> clasificacionns <- kmedias(datos, num_clusters = 3, mostrar_proceso = TRUE)
\end{Sinput}
\begin{Soutput}
Iteración:  1 
Matriz de Distancias:
           [,1]      [,2]      [,3]
 [1,] 0.0000000 3.0207615 3.8078866
 [2,] 3.0207615 0.0000000 0.7905694
 [3,] 3.8078866 0.7905694 0.0000000
 [4,] 4.1382363 2.6925824 2.8504386
 [5,] 0.9013878 2.3048861 3.0923292
 [6,] 0.2500000 3.2500000 4.0388736
 [7,] 4.3732139 2.5495098 2.5739075
 [8,] 3.5794553 0.5590170 0.2500000
 [9,] 0.2500000 2.9261750 3.7165172
[10,] 4.4721360 2.8504386 2.9154759
[11,] 4.3011626 2.2638463 2.2360680
[12,] 0.7071068 2.3717082 3.1622777
[13,] 3.3541020 0.3535534 0.5000000
[14,] 4.5069391 3.2500000 3.4003676
[15,] 4.0311289 1.0606602 0.5000000
Clusters Asignados:
 [1] 1 2 3 2 1 1 2 3 1 2 3 1 2 2 3
Nuevos centroides: 
         [,1]     [,2]
[1,] 3.350000 4.200000
[2,] 1.291667 1.416667
[3,] 0.312500 2.375000
Iteración:  2 
Matriz de Distancias:
           [,1]      [,2]      [,3]
 [1,] 0.3354102 3.7925823 3.8308982
 [2,] 2.7681221 1.9116783 0.9782797
 [3,] 3.5584407 2.0433666 0.6987712
 [4,] 3.8029594 0.8090203 2.1695694
 [5,] 0.5700877 2.8918588 3.0188212
 [6,] 0.5000000 3.9433929 4.0412908
 [7,] 4.0388736 0.6679675 1.8760414
 [8,] 3.3241540 1.8952609 0.6281172
 [9,] 0.1581139 3.5922853 3.6980780
[10,] 4.1367258 0.9400428 2.2194101
[11,] 3.9702015 0.5086065 1.5372967
[12,] 0.4031129 3.0970977 3.1405861
[13,] 3.0923292 1.7702205 0.6525192
[14,] 4.1743263 1.3648616 2.7135367
[15,] 3.7566608 1.6858274 0.3365728
Clusters Asignados:
 [1] 1 3 3 2 1 1 2 3 1 2 2 1 3 2 3
Nuevos centroides: 
     [,1] [,2]
[1,] 3.35 4.20
[2,] 1.50 0.65
[3,] 0.30 2.95
Iteración:  3 
Matriz de Distancias:
           [,1]      [,2]       [,3]
 [1,] 0.3354102 4.3384905 3.55562934
 [2,] 2.7681221 2.7060118 0.54083269
 [3,] 3.5584407 2.7879204 0.30413813
 [4,] 3.8029594 0.2692582 2.63486243
 [5,] 0.5700877 3.4438351 2.81602557
 [6,] 0.5000000 4.4592600 3.78219513
 [7,] 4.0388736 0.2692582 2.39635139
 [8,] 3.3241540 2.6617663 0.07071068
 [9,] 0.1581139 4.1182521 3.45398321
[10,] 4.1367258 0.1500000 2.72809457
[11,] 3.9702015 0.6103278 2.07183494
[12,] 0.4031129 3.6704904 2.89698119
[13,] 3.0923292 2.5539186 0.20615528
[14,] 4.1743263 0.6403124 3.19061123
[15,] 3.7566608 2.3817011 0.54083269
Clusters Asignados:
 [1] 1 3 3 2 1 1 2 3 1 2 2 1 3 2 3
Nuevos centroides: 
     [,1] [,2]
[1,] 3.35 4.20
[2,] 1.50 0.65
[3,] 0.30 2.95
Convergencia alcanzada en la iteración: 3 
\end{Soutput}
\begin{Sinput}
> # Añadimos la columna de clusters a los datos originales
> m <- cbind(clasificacionns$cluster, datos)
> # Separamos la matriz por clusters
> mc1 <- subset(m, m[, 1] == 1)
> mc2 <- subset(m, m[, 1] == 2)
> mc3 <- subset(m, m[, 1] == 3)
> # Limpiamos los datos (eliminamos la columna de cluster)
> mc1 <- mc1[, -1]
> mc2 <- mc2[, -1]
> mc3 <- mc3[, -1]
> # Datos originales
> print(datos)
\end{Sinput}
\begin{Soutput}
   Velocidad Temperatura
1       3.50        4.50
2       0.75        3.25
3       0.00        3.00
4       1.75        0.75
5       3.00        3.75
6       3.75        4.50
7       1.25        0.75
8       0.25        3.00
9       3.50        4.25
10      1.50        0.50
11      1.00        1.00
12      3.00        4.00
13      0.50        3.00
14      2.00        0.25
15      0.00        2.50
\end{Soutput}
\begin{Sinput}
> # Resultado del K-Means
> print(m)
\end{Sinput}
\begin{Soutput}
   clasificacionns$cluster Velocidad Temperatura
1                        1      3.50        4.50
2                        3      0.75        3.25
3                        3      0.00        3.00
4                        2      1.75        0.75
5                        1      3.00        3.75
6                        1      3.75        4.50
7                        2      1.25        0.75
8                        3      0.25        3.00
9                        1      3.50        4.25
10                       2      1.50        0.50
11                       2      1.00        1.00
12                       1      3.00        4.00
13                       3      0.50        3.00
14                       2      2.00        0.25
15                       3      0.00        2.50
\end{Soutput}
\begin{Sinput}
> # Datos en el cluster 1
> print(mc1)
\end{Sinput}
\begin{Soutput}
   Velocidad Temperatura
1       3.50        4.50
5       3.00        3.75
6       3.75        4.50
9       3.50        4.25
12      3.00        4.00
\end{Soutput}
\begin{Sinput}
> #Datos en el cluster 2
> print(mc2)
\end{Sinput}
\begin{Soutput}
   Velocidad Temperatura
4       1.75        0.75
7       1.25        0.75
10      1.50        0.50
11      1.00        1.00
14      2.00        0.25
\end{Soutput}
\begin{Sinput}
> # Datos en el cluster 3
> print(mc3)
\end{Sinput}
\begin{Soutput}
   Velocidad Temperatura
2       0.75        3.25
3       0.00        3.00
8       0.25        3.00
13      0.50        3.00
15      0.00        2.50
\end{Soutput}
\begin{Sinput}
> 
> 
\end{Sinput}
\end{Schunk}
	\end{itemize}

	\newpage
	\subsubsection{Clusterización Jerárquica Aglomerativa}
	
	El segundo conjunto de datos, que se empleará para realizar el análisis de clasificación no supervisada con Clusterización Jerárquica Aglomerativa, será el mismo que el utilizado en el ejercicio anterior, por lo tanto estará formado por los siguientes 15 valores de velocidades de respuesta y temperaturas normalizadas de un microprocesador \{Velocidad, Temperatura\}: 1.\{3.5, 4.5\}; 2.\{0.75, 3.25\}; 3.\{0, 3\}; 4.\{1.75, 0.75\}; 5.\{3, 3.75\}; 6.\{3.75, 4.5\}; 7.\{1.25, 0.75\}; 8.\{0.25, 3\}; 9.\{3.5, 4.25\}; 10.\{1.5, 0.5\}; 11.\{1, 1\}; 12.\{3, 4\}; 13.\{0.5, 3\}; 14.\{2, 0.25\}; 15.\{0, 2.5\}. Del análisis visual de los datos se ha concluido que hay una alta probabilidad que sean tres clusters.
	
	\paragraph{Solución:}
	\begin{itemize}
		\item \texttt{datos <- read.csv("Datos2.2.csv")}: Se leen los datos del archivo .csv y se guardan en la variable "datos".
		\item \texttt{(datos<-t(datos))}: Se hace la traspuesta de la matriz datos para un mejor tratamiento.
		\item \texttt{(clMin<-agglomerative\_clustering(datos,''single´´,FALSE,FALSE))}: Clusterización usando como definición de proximidad, la distancia entre los puntos más cercanos de los clusters.
		\begin{itemize}
			\item [-] Parámetros:
			\begin{itemize}
				\item \texttt{datos}: Variable que contiene los puntos a evaluar.
				\item \texttt{"single"}: Algoritmo de clusterización, en este caso usando minimum linkage.
				\item \texttt{details="FALSE"}: Booleano para determinar si se imprimen o no los logs con la explicación de todo el proceso.
				\item \texttt{waiting="FALSE"}: Booleano para determinar si tiene que esperar input del usuario para ir imprimiendo los logs.
			\end{itemize}
			\item [-] Salida: Matriz con los clusters y sus distancias.
			\item [-] Explicación: Realiza un análisis jerárquico aglomerativo de los datos introducidos por parámetro. Repite una serie de pasos hasta que solo quede un cluster. Inicialmente cada punto se asigna a su propio cluster y se calcula la matriz de distancias entre ellos. Después se calcula la proximidad entre dos clusters con la distancia entre los 2 puntos más cercanos de dichos clusters (falta formula)
\begin{Schunk}
\begin{Sinput}
> library("clustlearn")
> datos <- read.csv("Datos2.2.csv")
> (datos<-t(datos))
\end{Sinput}
\begin{Soutput}
       [,1]
X3.5   4.50
X0.75  3.25
X0     3.00
X1.75  0.75
X3     3.75
X3.75  4.50
X1.25  0.75
X0.25  3.00
X3.5.1 4.25
X1.5   0.50
X1     1.00
X3.1   4.00
X0.5   3.00
X2     0.25
X0.1   2.50
\end{Soutput}
\begin{Sinput}
> (clMin<-agglomerative_clustering(datos,"single",FALSE,FALSE))
\end{Sinput}
\begin{Soutput}
Cluster method   : single 
Distance         : Euclidean 
Number of objects: 15 
\end{Soutput}
\end{Schunk}
		\end{itemize}
		
		\item \texttt{(clMax<-agglomerative\_clustering(datos,''complete´´,FALSE,FALSE))}: Clusterización usando como definición de proximidad, la distancia que haya entre los dos puntos más lejanos de los clusters.
		\begin{itemize}
			\item [-] Parámetros:
			\begin{itemize}
				\item \texttt{datos}: Variable que contiene los puntos a evaluar.
				\item \texttt{"single"}: Algoritmo de clusterización, en este caso usando maximum linkage.
				\item \texttt{details="FALSE"}: Booleano para determinar si se imprimen o no los logs con la explicación de todo el proceso.
				\item \texttt{waiting="FALSE"}: Booleano para determinar si tiene que esperar input del usuario para ir imprimiendo los logs.
			\end{itemize}
			\item [-] Salida: Matriz con los clusters y sus distancias.
			\item [-] Explicación: Realiza un análisis jerárquico aglomerativo de los datos introducidos por parámetro. Repite una serie de pasos hasta que solo quede un cluster. Inicialmente cada punto se asigna a su propio cluster y se calcula la matriz de distancias entre ellos. Después se calcula la proximidad entre dos clusters con la distancia entre los puntos más lejanos de dichos clusters (falta formula)
\begin{Schunk}
\begin{Sinput}
> install.packages("clustlearn")
> library("clustlearn")
> datos <- read.csv("Datos2.2.csv")
> (datos<-t(datos))
\end{Sinput}
\begin{Soutput}
       [,1]
X3.5   4.50
X0.75  3.25
X0     3.00
X1.75  0.75
X3     3.75
X3.75  4.50
X1.25  0.75
X0.25  3.00
X3.5.1 4.25
X1.5   0.50
X1     1.00
X3.1   4.00
X0.5   3.00
X2     0.25
X0.1   2.50
\end{Soutput}
\begin{Sinput}
> (clMin<-agglomerative_clustering(datos,"complete",FALSE,FALSE))
\end{Sinput}
\begin{Soutput}
Cluster method   : complete 
Distance         : Euclidean 
Number of objects: 15 
\end{Soutput}
\end{Schunk}
		\end{itemize}
		
		\item \texttt{(clAvg<-agglomerative\_clustering(datos,''average´´,FALSE,FALSE))}: Clusterización usando como definición de proximidad, la media de distancias entre todas las parejas de puntos de los dos clusters.
		\begin{itemize}
			\item [-] Parámetros:
			\begin{itemize}
				\item \texttt{datos}: Variable que contiene los puntos a evaluar.
				\item \texttt{"average"}: Algoritmo de clusterización, en este caso usando average linkage.
				\item \texttt{details="FALSE"}: Booleano para determinar si se imprimen o no los logs con la explicación de todo el proceso.
				\item \texttt{waiting="FALSE"}: Booleano para determinar si tiene que esperar input del usuario para ir imprimiendo los logs.
			\end{itemize}
			\item [-] Salida: Matriz con los clusters y sus distancias.
			\item [-] Explicación: Realiza un análisis jerárquico aglomerativo de los datos introducidos por parámetro. Repite una serie de pasos hasta que solo quede un cluster. Inicialmente cada punto se asigna a su propio cluster y se calcula la matriz de distancias entre ellos. Después se calcula la proximidad entre dos clusters con la media de todos los puntos de ambos (falta formula)
\begin{Schunk}
\begin{Sinput}
> library("clustlearn")
> datos <- read.csv("Datos2.2.csv")
> (datos<-t(datos))
\end{Sinput}
\begin{Soutput}
       [,1]
X3.5   4.50
X0.75  3.25
X0     3.00
X1.75  0.75
X3     3.75
X3.75  4.50
X1.25  0.75
X0.25  3.00
X3.5.1 4.25
X1.5   0.50
X1     1.00
X3.1   4.00
X0.5   3.00
X2     0.25
X0.1   2.50
\end{Soutput}
\begin{Sinput}
> (clMin<-agglomerative_clustering(datos,"average",FALSE,FALSE))
\end{Sinput}
\begin{Soutput}
Cluster method   : average 
Distance         : Euclidean 
Number of objects: 15 
\end{Soutput}
\end{Schunk}
		\end{itemize}
	\end{itemize}
	
	\subsection{Análisis de clasificación supervisada}
	
	\subsubsection{Árboles de decisión}
	
	El tercer conjunto de datos, que se empleará para realizar el análisis de clasificación supervisada utilizando árboles de decisión, estará formado por el siguiente conjunto de 10 sucesos constituidos por los valores de cuatro características de vehículos: 1.\{B,4,5,Coche\}; 2.\{A,2,2,Moto\}; 3.\{N,2,1,Bicicleta\}; 4.\{B,6,4,Camión\}; 5.\{B,4,6,Coche\}; 6.\{B,4,4,Coche\}; 7.\{N,2,2,Bicicleta\}; 8.\{B,2,1,Moto\}; 9.\{B,6,2,Camión\}; 10.\{N,2,1,Bicicleta\}, donde las características de cada suceso son: \{TipoCarnet, NúmeroRuedas, NúmeroPasajeros, TipoVehículo\}. Se debe clasificar el tipo de vehículo en función del resto de características. TipoCarnet, es el tipo de carnet necesario para conducir el vehículo.
	
	\paragraph{Solución:}
	\begin{itemize}
		\item Primero instalamos la libreria ''readr´´ y se leen los datos del archivo .csv para guardarlos en la variable ''datos´´.
\begin{Schunk}
\begin{Sinput}
> library(readr)
> (datos=read.csv('Datos2.3.csv'))
\end{Sinput}
\begin{Soutput}
   TipoCarnet NumeroRuedas NumeroPasajeros TipoVehiculo
1           B            4               5        Coche
2           A            2               2         Moto
3           N            2               1    Bicicleta
4           B            6               4       Camion
5           B            4               6        Coche
6           B            4               4        Coche
7           N            2               2    Bicicleta
8           B            2               1         Moto
9           B            6               2       Camion
10          N            2               1    Bicicleta
\end{Soutput}
\end{Schunk}
		\item Después de importar los datos pasamos los datos que no son tipo Integer, a tipo factor, ya que si no no podremos realizar el análisis correctamente.
\begin{Schunk}
\begin{Sinput}
> TipoCarnet=factor(datos$TipoCarnet)
> TipoVehiculo=factor(datos$TipoVehiculo)
\end{Sinput}
\end{Schunk}
		\item La variable tipo factor es de tipo categórica usada asiduamente para clasificaciones de datos. Sus valores se almacenan internamente en R como números enteros. Tras al conversión de tipos, creamos un dataframe con las nuevas variables y con las que no han sido cambiadas.
\begin{Schunk}
\begin{Sinput}
> muestra=data.frame(TipoCarnet=TipoCarnet, NumeroRuedas=datos$NumeroRuedas, NumeroPasajeros=datos$NumeroPasajeros, TipoVehiculo=TipoVehiculo)
\end{Sinput}
\end{Schunk}
    \item Una vez hecho esto procedemos a instalar el paquete ''tree´´, el cual construye árboles de decisión y haciendo uso de la función \texttt{tree()}. La función tree recibe la variable sobre la que queremos predecir, en este caso \texttt(TipoVehiculo), además de usar ''.´´ para que prediga respecto al resto de variables. También recibe el dataset que hemos creado previamente y del cual obtiene todos los datos. Por último recibe la función para que el árbol se ajuste a los datos en cuestión, \texttt{tree.control()}, que recibe los parámetros el número de observaciones (10), \texttt{minsize=2} y \texttt{mindev=0}.
\begin{Schunk}
\begin{Sinput}
> library(tree)
> arbol <- tree(TipoVehiculo ~ ., data=muestra, control=tree.control(10,minsize=2,mindev=0))
\end{Sinput}
\end{Schunk}
		\item Para finalizar visualizamos el árbol resultante con \texttt{print()}, muestra las variables de los nodos internos y  los nodos terminales.
\begin{Schunk}
\begin{Sinput}
> print(arbol)
\end{Sinput}
\begin{Soutput}
node), split, n, deviance, yval, (yprob)
      * denotes terminal node

1) root 10 27.32 Bicicleta ( 0.3 0.2 0.3 0.2 )  
  2) NumeroRuedas < 3 5  6.73 Bicicleta ( 0.6 0.0 0.0 0.4 )  
    4) TipoCarnet: A,B 2  0.00 Moto ( 0.0 0.0 0.0 1.0 ) *
    5) TipoCarnet: N 3  0.00 Bicicleta ( 1.0 0.0 0.0 0.0 ) *
  3) NumeroRuedas > 3 5  6.73 Coche ( 0.0 0.4 0.6 0.0 )  
    6) NumeroRuedas < 5 3  0.00 Coche ( 0.0 0.0 1.0 0.0 ) *
    7) NumeroRuedas > 5 2  0.00 Camion ( 0.0 1.0 0.0 0.0 ) *
\end{Soutput}
\end{Schunk}
		\end{itemize}
	
	\subsubsection{Regresión}
	
	El cuarto conjunto de datos, que se empleará para realizar el análisis de clasificación supervisada utilizando regresión, estará formado por los siguientes 4 subconjuntos de datos: 1.\{10, 8.04; 8, 6.95; 13, 7.58; 9, 8.81; 11, 8.33; 14, 9.96; 6, 7.24; 4, 4.26; 12, 10.84; 7, 4.82; 5, 5.68\}; 2.\{10, 9.14; 8, 8.14; 13, 8.74; 9, 8.77; 11, 9.26; 14, 8.1; 6, 6.13; 4, 3.1; 12, 9.13; 7, 7.26; 5, 4.74\}; 3.\{10, 7.46; 8, 6.77; 13, 12.74; 9, 7.11; 11, 7.81; 14, 8.84; 6, 6.08; 4, 5.39; 12, 8.15; 7, 6.42; 5, 5.73\}; 4.\{8, 6.58; 8, 5.76; 8, 7.71; 8, 8.84; 8, 8.47; 8, 7.04; 8, 5.25; 19, 12.5; 8, 5.56; 8, 7.91; 8, 6.89\}. Se deben calcular las rectas de regresión de los cuatro subconjuntos y sus parámetros de ajuste.
		
	\paragraph{Solución:}
			Algoritmo de minería de reglas de asociación (apriori): Su objetivo principal es descubrir patrones de asociación entre diferentes conjuntos de datos.
			\begin{itemize}
				\item[-] Para empezar tenemos que introducir los datos en un archivo con extensión .txt 
\begin{Schunk}
\begin{Sinput}
> source("regresion_lineal.R")
> source("R-squared.R")
> # Leer el archivo del primer subconjunto de datos
> muestra <- read.table("datos2.4.1.txt")
> muestra2 <- read.table("datos2.4.2.txt")
> muestra3 <- read.table("datos2.4.3.txt")
> muestra4 <- read.table("datos2.4.4.txt")
> # Llamar a la función de regresión lineal
> resultado <- regresion_lineal(muestra$x,muestra$y)
> resultado2 <- regresion_lineal(muestra2$x,muestra2$y)
> resultado3 <- regresion_lineal(muestra3$x,muestra3$y)
> resultado4 <- regresion_lineal(muestra4$x,muestra4$y)
> # Mostrar los coeficientes de las rectas
> resultado
\end{Sinput}
\begin{Soutput}
$alpha
[1] 3.000091

$beta
[1] 0.5000909
\end{Soutput}
\begin{Sinput}
> resultado2
\end{Sinput}
\begin{Soutput}
$alpha
[1] 3.000909

$beta
[1] 0.5
\end{Soutput}
\begin{Sinput}
> resultado3
\end{Sinput}
\begin{Soutput}
$alpha
[1] 3.002455

$beta
[1] 0.4997273
\end{Soutput}
\begin{Sinput}
> resultado4
\end{Sinput}
\begin{Soutput}
$alpha
[1] 3.001727

$beta
[1] 0.4999091
\end{Soutput}
\begin{Sinput}
> # Llamar a la función de R-squared
> Rsquared <- calculo_R_cuadrado(muestra$x,muestra$y,resultado$alpha,resultado$beta)
> Rsquared2 <- calculo_R_cuadrado(muestra2$x,muestra2$y,resultado2$alpha,resultado2$beta)
> Rsquared3 <- calculo_R_cuadrado(muestra3$x,muestra3$y,resultado3$alpha,resultado3$beta)
> Rsquared4 <- calculo_R_cuadrado(muestra4$x,muestra4$y,resultado4$alpha,resultado4$beta)
> #Mostrar bonanza regresión
> print(paste("R^2:", Rsquared))
\end{Sinput}
\begin{Soutput}
[1] "R^2: 0.666542459508774"
\end{Soutput}
\begin{Sinput}
> print(paste("R^2:", Rsquared2))
\end{Sinput}
\begin{Soutput}
[1] "R^2: 0.666242033727482"
\end{Soutput}
\begin{Sinput}
> print(paste("R^2:", Rsquared3))
\end{Sinput}
\begin{Soutput}
[1] "R^2: 0.666324041066559"
\end{Soutput}
\begin{Sinput}
> print(paste("R^2:", Rsquared4))
\end{Sinput}
\begin{Soutput}
[1] "R^2: 0.666707256898466"
\end{Soutput}
\end{Schunk}
				
				\item[-] Retorno:
				\begin{itemize}
				  \item La función \texttt{regresion\_lineal()} retorna una lista con los coeficientes alpha y beta.
				  \item La función \texttt{calculo\_R\_cuadrado()} retorna un double con el valor del ajuste (R-squared).
				  \item La función \texttt{calculoSSR()} retorna un double con la dispersión de y calculados.
				  \item La función \texttt{calculoSSy()} retorna un double con la dispersión de y observados.
				\end{itemize}
				
				\item[-] Explicación: La función \texttt{regresion\_lineal()}, calcula los coeficientes alpha y beta de la recta que forman los datos introducidos por parámetros. Después la función \texttt{calculo\_R\_cuadrado()} recibe como parámetros dichos coeficientes más los datos, con ello hace uso de las funciones \texttt{calculoSSR()} (que calcula la dispersión de los valores de y calculados a través de la recta creada con alpha y beta) y \texttt{calculoSSy()} (que calcula la dispersión de los valores y observados de los datos iniciales) para calcular R-squared, que se mide entre 0 y 1 y muestra el ajuste de la recta sobre los datos iniciales.

			\end{itemize}
\end{document}
